{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_Optimization_Methods.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb2eWxK5G9Pg",
        "colab_type": "text"
      },
      "source": [
        "*Homework 03*\n",
        "\n",
        "Kirill Shcherbakov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemuMJ2bmTh1",
        "colab_type": "text"
      },
      "source": [
        "**Question 01**\n",
        "\n",
        "First of all, by the definition of the convexity (where $\\alpha \\in [0,1]$):\n",
        "\n",
        "$$f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)$$\n",
        "\n",
        "Let us rewrite function $h(x)$ in the following form:\n",
        "\n",
        "$$h(\\alpha x + (1-\\alpha)y) = \\max{{f(\\alpha x + (1-\\alpha)y),g(\\alpha x + (1-\\alpha)y)}} = \\begin{cases} f(\\alpha x + (1-\\alpha)y), f(\\alpha x + (1-\\alpha)y) \\geq g(\\alpha x + (1-\\alpha)y) \\\\ g(\\alpha x + (1-\\alpha)y), f(\\alpha x + (1-\\alpha)y) < g(\\alpha x + (1-\\alpha)y)\\end{cases} $$\n",
        "\n",
        "And using the convexity definition:\n",
        "\n",
        "$$\\begin{cases} f(\\alpha x + (1-\\alpha)y), f(\\alpha x + (1-\\alpha)y) \\geq g(\\alpha x + (1-\\alpha)y) \\\\ g(\\alpha x + (1-\\alpha)y), f(\\alpha x + (1-\\alpha)y) < g(\\alpha x + (1-\\alpha)y)\\end{cases} \\leq \\begin{cases} \\alpha f(x)+(1-\\alpha)f(y), f(\\alpha x + (1-\\alpha)y) \\geq g(\\alpha x + (1-\\alpha)y) \\\\ \\alpha g(x)+(1-\\alpha)g(y), f(\\alpha x + (1-\\alpha)y) < g(\\alpha x + (1-\\alpha)y)\\end{cases}$$\n",
        "\n",
        "In other words, we obtained:\n",
        "\n",
        "$$\\alpha f(x)+(1-\\alpha)f(y) \\leq \\alpha\\max(f(x),g(x))+(1-\\alpha)\\max(f(y),g(y)) = \\alpha h(x)+(1-\\alpha)h(y)$$\n",
        "\n",
        "In the same way we obtain the following:\n",
        "\n",
        "$$\\alpha g(x)+(1-\\alpha)g(y) \\leq \\alpha\\max(f(x),g(x))+(1-\\alpha)\\max(f(y),g(y)) = \\alpha h(x)+(1-\\alpha)h(y)$$\n",
        "\n",
        "And finally we have:\n",
        "\n",
        "$$h(\\alpha x + (1-\\alpha)y) \\leq \\begin{cases} \\alpha f(x)+(1-\\alpha)f(y), f(\\alpha x + (1-\\alpha)y) \\geq g(\\alpha x + (1-\\alpha)y) \\\\ \\alpha g(x)+(1-\\alpha)g(y), f(\\alpha x + (1-\\alpha)y) < g(\\alpha x + (1-\\alpha)y)\\end{cases} \\leq \\begin{cases} \\alpha h(x)+(1-\\alpha)h(y), f(\\alpha x + (1-\\alpha)y) \\geq g(\\alpha x + (1-\\alpha)y) \\\\ \\alpha h(x)+(1-\\alpha)h(y), f(\\alpha x + (1-\\alpha)y) < g(\\alpha x + (1-\\alpha)y)\\end{cases} = \\alpha h(x)+(1-\\alpha)h(y)$$\n",
        "\n",
        "and obtained:\n",
        "\n",
        "$$h(\\alpha x + (1-\\alpha)y) \\leq \\alpha h(x)+(1-\\alpha)h(y)$$\n",
        "\n",
        "which proves that $h(x)$ is convex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQlhKlWcmZiA",
        "colab_type": "text"
      },
      "source": [
        "**Question 02**\n",
        "\n",
        "We have $f(x) = (Ax, x)$. Let's consider the quadratic form as the following: \n",
        "\n",
        "$$f(x)=x^T A x$$.\n",
        "\n",
        "The gradient is: \n",
        "\n",
        "$$ \\nabla_x {f} = x^T (A + A^T) $$\n",
        "\n",
        "Hessian is:\n",
        "\n",
        "$$ \\nabla_x^2 {f} = A + A^T $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMrXbWAxoEgM",
        "colab_type": "text"
      },
      "source": [
        "**Question 03**\n",
        "\n",
        "Here $\\bar{h}$ denotes the extended-value of the function $h$, which assigns the value $\\infty$ to points not in dom $h$ for convex. To say that $\\bar{h}$ is non-increasing means that that for any $x, y \\in R$, with $ x < y$ we have $\\bar{h(x)}  \\geq \\bar{h(y)} $.\n",
        "\n",
        "Let us prove the decomposition rule directly, without assuming differentiability. Without loss of generality we can assume that $n=1$. Assume that $x,y \\in dom f$ and $\\alpha \\in [0,1]$. Since $x,y \\in dom f$, we have that $x,y \\in dom g$ and $g(x), g(y) \\in dom h$. Since $dom g$ is concave, we conclude that $(1-\\alpha) x + \\alpha y \\in dom g$, and from concavity of g, we have:\n",
        "\n",
        "$$ g((1 - \\alpha)x + \\alpha y) \\geq (1-\\alpha)g(x) + \\alpha g(y), \\forall x,y \\in R$$\n",
        "\n",
        "Since $g(x), g(y) \\in dom h$, we conclude that $(1-\\alpha) g(x) + \\alpha g(y) \\in dom h$, i.e., the righthand side of our previous equation is in $dom h$. Now we use assumption that $\\bar{h}$ is non-increasing. We conclude that the lefthand side, i.e., $g((1 - \\alpha)x + \\alpha y) \\in dom h$. This means that $(1 - \\alpha)x + \\alpha y \\in domf$. At this point we shown that $dom f$ is convex. \n",
        "\n",
        "Now using the fact that $\\bar{h}$ is non-increasing and the condition of concavity of g, we get:\n",
        "\n",
        "$$h(g((1 - \\alpha)x + \\alpha y)) \\leq h((1-\\alpha)g(x) + \\alpha g(y)), \\forall x,y \\in R$$\n",
        "\n",
        "From convexity of h, we have \n",
        "\n",
        "$$ h((1 - \\alpha)g(x) + \\alpha g(y)) \\leq (1-\\alpha)h(g(x))  + \\alpha h(g(y)) $$\n",
        "\n",
        "Putting together, we have two equations above\n",
        "\n",
        "$$ h(g((1 - \\alpha)x + \\alpha y)) \\leq (1-\\alpha)h(g(x))  + \\alpha h(g(y)) $$\n",
        "\n",
        "which proves the composition theorem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCoECKGi7BMp",
        "colab_type": "text"
      },
      "source": [
        "**Question 4**\n",
        "\n",
        "According to the definition of the strongly convex function:\n",
        "\n",
        "$$\n",
        "f(y) \\geq f(x) + \\nabla f(x)^T (y-x) + \\frac{m}{2} \\||y-x\\||_2^2, \\forall x,y \\in dom(f)\n",
        "$$\n",
        "\n",
        "If there exist $c = c^*$ such that $Q(c^*)$ is unbounded then $\\forall x$: $ c^* \\geq f(x)$\n",
        "\n",
        "We choose $x = x^*$ such that $\\|\\nabla f(x^*)\\|_2^2 > g > 0$.\n",
        "\n",
        "Let us determine $f(x^*) = f_0$ and choose $y = x^* - \\lambda\\nabla f(x^*)$, where $\\lambda > 0$.\n",
        "\n",
        "Then putting it together in the strongly convexity equation we have:\n",
        "\n",
        "$$f(y) \\geq f(x^*) + \\nabla f(x)^T (x^*-x^*+\\lambda \\nabla f(x^*)) + \\frac{m}{2}\\|x^*-x^*+\\lambda \\nabla f(x^*)\\|_2^2$$\n",
        "\n",
        "$$f(y) \\geq f_0 + \\lambda\\|\\nabla f(x^*)\\|_2^2 + \\lambda^2\\frac{m}{2}\\|\\nabla f(x^*)\\|_2^2$$\n",
        "\n",
        "$$c^*\\geq f(y) \\geq f_0 + \\lambda\\|\\nabla f(x^*)\\|_2^2 + \\lambda^2\\frac{m}{2}\\|\\nabla f(x^*)\\|_2^2$$\n",
        "\n",
        "Using $\\forall x$: $ c^* \\geq f(x)$:\n",
        "\n",
        "$$\\frac{mg}{2}\\lambda^2 + g\\lambda + f_0-c^* \\leq 0$$\n",
        "\n",
        "Our parabolic function is  obviously less than zero $(mg > 0)$ Not for all  $\\lambda$.\n",
        "\n",
        "Hence, we get a contradiction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsHkET-17BSZ",
        "colab_type": "text"
      },
      "source": [
        "**Question 05**\n",
        "\n",
        "The epigraph of a function by the definition is:\n",
        "\n",
        "$f: x \\rightarrow [-\\infty, \\infty]$ is the subset of $R^{n+1}$ given by\n",
        "\n",
        "$$\n",
        "epi(f)= (x,w)|x \\in X, w \\in R, f(x) \\leq w\n",
        "$$\n",
        "\n",
        "Hence, the dimenstions of $c$ must be (n, 1) and dimension of b is (1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucL1IONB7Bzb",
        "colab_type": "text"
      },
      "source": [
        "**Question 06**\n",
        "\n",
        "We have\n",
        "\n",
        "$$f(x) = (Qx,x) = x^TQx$$\n",
        "\n",
        "The conjugate function by the definition is $$f^*(y) = sup_{x \\in domf}( y^Tx - f(x))$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{df^*(y)}{dx} = y - x^T (Q + Q^T) = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "x = (Q+Q^T)^{-1}y\n",
        "$$\n",
        "\n",
        "Hence the conjugate function is:\n",
        "\n",
        "$$\n",
        "f^*(y) = y^T(Q+Q^T)^{-1}y - y^T(Q+Q^T)^{-1}Q(Q+Q^T)^{-1}\n",
        "y$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnjYlN09_CFw",
        "colab_type": "text"
      },
      "source": [
        "**Question 7**\n",
        "\n",
        "We have to solve the following one-dimensional optimization problem:\n",
        "\n",
        "$$\\min\\limits_{z \\in R} \\frac{1}{m} \\sum \\limits_{i=1}^{m} (z-x_i)^2$$\n",
        "\n",
        "Let's calculate the derivative, and equal it to zero in order to find the solution:\n",
        "\n",
        "$$\\frac{1}{m} \\sum \\limits_{i=1}^{m2} (z - x_{i}) = 0$$\n",
        "\n",
        "$$\\sum \\limits_{i=1}^{m} (z-x_i)  = mz - \\sum \\limits_{i=1}^{mx_i} = 0$$\n",
        "\n",
        "Finally, we obtain:\n",
        "\n",
        "$$z = \\frac{1}{m} \\sum \\limits_{i=1}^{m} x_i$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT-1plZ0_CIT",
        "colab_type": "text"
      },
      "source": [
        "**Question 08**\n",
        "\n",
        "We have:\n",
        "\n",
        "$$ \\min_{a,b} \\frac{1}{m} \\sum_{i=1}^{m} (a x_i + b - y_i)^2 $$\n",
        "\n",
        "Let's find the deriv. with respect to a and b:\n",
        "\n",
        "$$\n",
        "\\frac{df}{da} = \\frac{2}{m} \\sum_{i=1}^{m} (a x_i^2 + b x_i - y_i x_i) = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{df}{db} = \\frac{2}{m} \\sum_{i=1}^{m} (a x_i + b - y_i) = 0\n",
        "$$\n",
        "\n",
        "Let's denote $\\bar{y} = \\frac{y_1 + y_2 + y_3 + ... + y_m}{m}, \\bar{x} = \\frac{x_1 + x_2 + x_3 + ... + x_m}{m}$.\n",
        "\n",
        "From the second equation we obtain:\n",
        "\n",
        "$$\\sum_{i=1}^{m} (a x_i + b - y_i) = ma\\bar{x}+mb-m\\bar{y} = 0$$\n",
        "\n",
        "$$a\\bar{x}+b-\\bar{y} = 0$$\n",
        "\n",
        "$$b = \\bar{y}-a\\bar{x}$$\n",
        "\n",
        "And now let's consider the first equation in a vector form:\n",
        "\n",
        "$$a(x,x) + m(\\bar{y}-a\\bar{x})\\bar{x}-(y,x) = 0$$\n",
        "\n",
        "$$a = \\frac{(y,x)-m\\bar{y}\\bar{x}}{(x,x)-m\\bar{x}^2}$$\n",
        "\n",
        "And\n",
        "\n",
        "$$b = \\bar{y}-\\bar{x}\\frac{(y,x)-m\\bar{y}\\bar{x}}{(x,x)-m\\bar{x}^2}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BMguGczMSXk",
        "colab_type": "text"
      },
      "source": [
        "**Question 9**\n",
        "\n",
        "First, let us prove that $H_{k+1}$ satisfies quasi-Newton conditions:\n",
        "\n",
        " $$H_{k+1} = \\left(I-\\frac{sd^T}{(d,s)}\\right)H_k\\left(I-\\frac{ds^T}{(d,s)} \\right) + \\frac{ss^T}{(d,s)}$$\n",
        "\n",
        "In other words, we have to prove that it satisfies the following: $s = H_{k+1}d$\n",
        "\n",
        "Let's multiply $H_{k+1}$ by $s$. Hence, we obtain:\n",
        "\n",
        "$$H_{k+1}d = \\left(I-\\frac{sd^T}{(d,s)}\\right)H_k\\left(d-\\frac{ds^Td}{(d,s)} \\right) + \\frac{ss^Td}{(d,s)} = \\left(I-\\frac{sd^T}{(d,s)}\\right)H_k\\left(d-d \\right) + s = s$$\n",
        "\n",
        "Now, let's prove the following:\n",
        "\n",
        "$$B_{k+1} = B_k + \\frac{dd^T}{(d,s)}-\\frac{B_kss^TB_k}{(B_ks,s)}$$\n",
        "\n",
        "We have prove that it satisfies the following: $d = B_{k+1}s$\n",
        "\n",
        "Let's multiply $B_{k+1}$ by $s$:\n",
        "\n",
        "$$B_{k+1}s = B_ks + \\frac{dd^Ts}{(d,s)}-\\frac{B_kss^TB_ks}{(B_ks,s)} = B_ks + d-\\frac{B_ks(B_ks,s)}{(B_ks,s)} = d$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV0e6Gd16rJb",
        "colab_type": "text"
      },
      "source": [
        "**Question 10** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bd2678bf-b8f8-4416-9e45-d60e1abb1347",
        "id": "XtNtHLA0OZvj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "pip install numdifftools"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numdifftools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/c0/b0d967160ecc8db52ae34e063937d85e8d386f140ad4826aae2086245a5e/numdifftools-0.9.39-py2.py3-none-any.whl (953kB)\n",
            "\u001b[K     |████████████████████████████████| 962kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: numdifftools\n",
            "Successfully installed numdifftools-0.9.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcW7YtnvDLJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numdifftools as nd\n",
        "\n",
        "def func(x):\n",
        "\n",
        "  return 100 * (x[1] - x[0] ** 2) ** 2 + (1 - x[0]) ** 2\n",
        "\n",
        "class multivariable_newton():\n",
        "\n",
        "    def __init__(self, func, start_point, step_size = 0.8, num_iter = 10000, tol = 1e-6):\n",
        "\n",
        "        self.func = func\n",
        "        self.start_point = np.array(start_point)\n",
        "        self.num_iter = num_iter\n",
        "        self.step_size = step_size\n",
        "        self.tol = tol\n",
        "        self.iters = 0\n",
        "\n",
        "    def newton_method(self):\n",
        "\n",
        "        x_t = self.start_point\n",
        "\n",
        "        #Hessian of function\n",
        "        H = nd.Hessian(self.func)\n",
        "\n",
        "        #Gradient of function\n",
        "        g = nd.Gradient(self.func)\n",
        "        grad = []\n",
        "\n",
        "        for _ in range(self.num_iter):\n",
        "            x_tplus1 = x_t - self.step_size * np.dot(np.linalg.pinv(H(x_t)), g(x_t))\n",
        "            self.iters += 1\n",
        "            grad.append(g(x_t))\n",
        "\n",
        "            #check for convergence\n",
        "            if abs(max(x_tplus1 - x_t)) < self.tol:\n",
        "                break\n",
        "            x_t = x_tplus1\n",
        "\n",
        "        else:\n",
        "            raise print(\"No convergence after %d iterations\" % (self.num_iter))\n",
        "\n",
        "        self.crit_point = x_tplus1\n",
        "        self.max_min = self.func(x_t)\n",
        "        self.grad = grad\n",
        "        return self\n",
        "\n",
        "    def critical_point(self):\n",
        "\n",
        "        return self.crit_point, self.iters, self.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lne8B2MgPZ53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ec02f32a-582e-4e44-b80d-ce3e4a78a94c"
      },
      "source": [
        "start_point = [2, -2]\n",
        "y = func(start_point)\n",
        "\n",
        "test = multivariable_newton(func, start_point)\n",
        "\n",
        "test.newton_method()\n",
        "crit_point, iters, grad = test.critical_point()\n",
        "print(\"Newton method\")\n",
        "print(\"===============\")\n",
        "print(\"we find critical point as:\", crit_point)\n",
        "print(\"it took the following number of iterations:\", iters)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Newton method\n",
            "===============\n",
            "we find critical point as: [1.00000008 1.00000015]\n",
            "it took the following number of iterations: 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7y4DOoKEmzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.optimize import line_search as line\n",
        "\n",
        "class BFGS_LS():\n",
        "\n",
        "    def __init__(self, func, start_point, grad, max_iter = 10000, tol = 1e-6):\n",
        "\n",
        "        self.grad = grad\n",
        "        self.func = func\n",
        "        self.x0 = start_point\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.iters = 0\n",
        "        self.x = [self.x0.copy()]\n",
        "\n",
        "\n",
        "    def BFGS(self):\n",
        "\n",
        "        n = len(self.x0)\n",
        "        stop_factor = np.inf\n",
        "        mat = np.eye(n)\n",
        "        cond = []\n",
        "\n",
        "        while stop_factor > self.tol and self.iters < self.max_iter:\n",
        "          grad_f = self.grad(self.x0)\n",
        "          d1 = - mat @ grad_f\n",
        "\n",
        "          #linesearch with the Wolfe conditions\n",
        "          gamma = line(self.func, self.grad, self.x0, d1)[0]\n",
        "          d2 = gamma * d1\n",
        "          d3 = self.grad(self.x0 + d2) - grad_f\n",
        "\n",
        "          #condition check\n",
        "          cond.append(d3 @ d2 > 0)\n",
        "          self.x0 = self.x0 + d2\n",
        "          stop_factor = np.linalg.norm(d2)\n",
        "          self.x.append(self.x0.copy())\n",
        "          self.iters += 1\n",
        "          \n",
        "          r = 1/(d2 @ d3)\n",
        "          mat1 = np.eye(n) - r * d2[:, np.newaxis] @ d3[np.newaxis,:]\n",
        "          mat2 = np.eye(n) - r * d3[:,np.newaxis] @ d2[np.newaxis,:]\n",
        "          mat = mat1 @ mat @ mat2 + r * d2[:,np.newaxis] @ d2[np.newaxis,:]\n",
        "\n",
        "        return self.x[-1], cond, self.iters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib83g-4UFJMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "31a773aa-6c00-4aac-b932-68820b425fe0"
      },
      "source": [
        "# BFGS performance\n",
        "test = BFGS_LS(func, start_point, nd.Gradient(func))\n",
        "point, conditions, iters = test.BFGS()\n",
        "print(\"BFGS quasi-Newton method\")\n",
        "print(\"===============\")\n",
        "print(\"we find critical point as:\", point)\n",
        "print(\"it took the following number of iterations:\", iters)\n",
        "print(\"condition check for each iteration:\", conditions)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BFGS quasi-Newton method\n",
            "===============\n",
            "we find critical point as: [1. 1.]\n",
            "it took the following number of iterations: 39\n",
            "condition check for each iteration: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCBa27Pwr59e",
        "colab_type": "text"
      },
      "source": [
        "When the Newton’s method converges, it converges very fast (quadratic convergence asymptotically). \n",
        "\n",
        "The Newton’s method requires second order derivatives which are difficult, if possible, to obtain. At very high-dimension, the inversion of the Hessian can be costly and unstable (large scale > 250). Furthermore, to store the second derivatives, we need $O(n^2)$ storage, where n is the number of variables of the objective function. The steepest descent method and quasi-Newton methods can be used instead. The quasi-Newton method is a good compromise between convergence speed and complexity. It usually converges fast, and some times converges even without step length control. The drawback is the high storage requirement.\n",
        "\n",
        "The performance of line search algorithms depends on many parameters, like the condition(s) you choose and constants in those conditions. Different line searching conditions could give different performance of you descent direction methods.\n",
        "\n",
        "For this problem, I think, Newton method is not well-defined, because there are points at which the Hessian becomes a singular, for instance, point (0, -1/100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tJXco_0S2NM",
        "colab_type": "text"
      },
      "source": [
        "**Question 11** \n",
        "\n",
        "Let us find $a$, $b$ and $c$ for our function and define her."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEC5pEXx5Tuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take_params(x):\n",
        "\n",
        "  a = np.ones((len(x), len(x)))\n",
        "  b = np.ones(len(x))\n",
        "  c = -np.ones(len(x))\n",
        "\n",
        "  return a, b, c\n",
        "\n",
        "def logfunc(x):\n",
        "\n",
        "  a, b, c = take_params(x)\n",
        "  f = c @ x - np.sum(np.log(b - a.T @ x))\n",
        "\n",
        "  return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54TlFRABaUV_",
        "colab_type": "text"
      },
      "source": [
        "***Case n=2***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJoXHk6QVLgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a28b9594-b9b1-4a4d-94ec-25ab1c362ea7"
      },
      "source": [
        "#parameters initialization\n",
        "n = 2\n",
        "x0 = np.zeros(n)\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action = 'ignore')\n",
        "\n",
        "#Newton method\n",
        "test2 = multivariable_newton(logfunc, x0)\n",
        "\n",
        "test2.newton_method()\n",
        "crit_point2, iters2, grad2 = test2.critical_point()\n",
        "print(\"Newton method for Task 11 our implementation for n=2:\")\n",
        "print(\"===============\")\n",
        "print(\"we find critical point as:\", crit_point2)\n",
        "print(\"it took the following number of iterations:\", iters2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Newton method for Task 11 our implementation for n=2:\n",
            "===============\n",
            "we find critical point as: [-0.49999987 -0.49999988]\n",
            "it took the following number of iterations: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1odg852Yb2a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0df24111-d3ec-41c0-9fba-4e5ea48f18de"
      },
      "source": [
        "# BFGS \n",
        "test3 = BFGS_LS(logfunc, x0, nd.Gradient(logfunc))\n",
        "\n",
        "point3, conditions3, iters3 = test3.BFGS()\n",
        "print(\"BFGS quasi-Newton method for Task 11 our implementation for n=2:\")\n",
        "print(\"===============\")\n",
        "print(\"we find critical point as:\", point3)\n",
        "print(\"it took the following number of iterations:\", iters3)\n",
        "print(\"condition check for each iteration:\", conditions3)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BFGS quasi-Newton method for Task 11 our implementation for n=2:\n",
            "===============\n",
            "we find critical point as: [-0.5 -0.5]\n",
            "it took the following number of iterations: 8\n",
            "condition check for each iteration: [True, True, True, True, True, True, True, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIm55yb1XUFo",
        "colab_type": "text"
      },
      "source": [
        "Let's compare with built-in SciPy realization our result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR24LE1xWFrh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cf6fffc9-38f0-44bd-907c-67a03f0d94e6"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "res_scipy = minimize(logfunc, x0)\n",
        "\n",
        "print(\"SciPy built-in for Task 11 built-in for n=2:\")\n",
        "print(\"===============\")\n",
        "print(\"we find critical point as:\", res_scipy.x)\n",
        "print(\"it took the following number of iterations:\", res_scipy.nit)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SciPy built-in for Task 11 built-in for n=2:\n",
            "===============\n",
            "we find critical point as: [-0.49999987 -0.49999987]\n",
            "it took the following number of iterations: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGxLDNeahll",
        "colab_type": "text"
      },
      "source": [
        "***Case n=10***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGw9JzSbXJPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "234c89a7-aead-4821-e793-6169487ceaaf"
      },
      "source": [
        "n2 = 10\n",
        "x0_2 = np.zeros(n2)\n",
        "\n",
        "#Newton method n=10\n",
        "test4 = multivariable_newton(logfunc, x0_2)\n",
        "\n",
        "test4.newton_method()\n",
        "crit_point4, iters4, grad4 = test4.critical_point()\n",
        "print(\"Newton method for Task 11 our implementation n=10:\")\n",
        "print(\"===============\")\n",
        "print(\"we find critical point as:\", crit_point4)\n",
        "print(\"it took the following number of iterations:\", iters4)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Newton method for Task 11 our implementation n=10:\n",
            "===============\n",
            "we find critical point as: [-0.774488   -0.78089669 -0.40223461 -1.61685863 -0.66109462 -1.25422435\n",
            " -0.92992321 -0.94620204 -0.68125072 -0.95257415]\n",
            "it took the following number of iterations: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIUs8qxra3qW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fbdbb390-30b9-4836-c442-4eca9034bcac"
      },
      "source": [
        "# BFGS n = 10\n",
        "test5 = BFGS_LS(logfunc, x0_2, nd.Gradient(logfunc))\n",
        "\n",
        "point5, conditions5, iters5 = test5.BFGS()\n",
        "print(\"BFGS quasi-Newton method for Task 11 our implementation n=10\")\n",
        "print(\"===============\")\n",
        "print(\"we find critical point as:\", point5)\n",
        "print(\"it took the following number of iterations:\", iters5)\n",
        "print(\"condition check for each iteration:\", conditions5)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BFGS quasi-Newton method for Task 11 our implementation n=10\n",
            "===============\n",
            "we find critical point as: [-0.9 -0.9 -0.9 -0.9 -0.9 -0.9 -0.9 -0.9 -0.9 -0.9]\n",
            "it took the following number of iterations: 8\n",
            "condition check for each iteration: [True, True, True, True, True, True, True, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBAoWp3AbT60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d4a37c9c-879c-4f18-c4d9-ba9cf1f5c8ee"
      },
      "source": [
        "res_scipy2 = minimize(logfunc, x0_2)\n",
        "\n",
        "print(\"SciPy built-in for Task 11 for n=10:\")\n",
        "print(\"===============\")\n",
        "print(\"we find critical point as:\", res_scipy2.x)\n",
        "print(\"it took the following number of iterations:\", res_scipy2.nit)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SciPy built-in for Task 11 for n=10:\n",
            "===============\n",
            "we find critical point as: [-0.90000019 -0.90000019 -0.90000019 -0.90000019 -0.90000019 -0.90000019\n",
            " -0.90000019 -0.90000019 -0.90000019 -0.90000019]\n",
            "it took the following number of iterations: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaX0SOsygZX0",
        "colab_type": "text"
      },
      "source": [
        "In case when n=10 we see that Newton method doesn't perform well. The reason could be in the evaluation of the Hessian of our function. But for BFGS quasi-Newton method the result is better than the SciPy built-in function (the number of iterations to converge is less)."
      ]
    }
  ]
}